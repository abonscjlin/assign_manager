#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·¥ä½œåˆ†é…ç®¡ç†ç³»çµ± - API Server
============================

æä¾›å¤–éƒ¨å‘¼å«çš„RESTful APIæ¥å£ï¼Œæ”¯æ´ï¼š
1. å·¥ä½œæ¸…å–®å’ŒæŠ€å¸«æ¸…å–®çš„å¤–éƒ¨è¼¸å…¥
2. è‡ªå‹•å·¥ä½œåˆ†é…è¨ˆç®—
3. çµæœè¼¸å‡ºï¼ˆåŒ…å«åŸå§‹è³‡æ–™ + åˆ†é…çµæœï¼‰

åŒæ™‚ä¿ç•™åŸæœ¬çš„CSVè®€å–åŠŸèƒ½ä¾›æœ¬åœ°æ¸¬è©¦ä½¿ç”¨ã€‚
"""

import sys
import os
import json
import pandas as pd
from datetime import datetime
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
import traceback

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°Pythonè·¯å¾‘ï¼ˆå›åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å°å…¥ç³»çµ±æ¨¡çµ„
from config_params import *
from strategy_manager import StrategyManager, get_strategy_manager
from employee_manager import EmployeeManager
from path_utils import get_data_file_path
from update_assignment_results import assign_workers_to_tasks

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å»ºç«‹Flaskæ‡‰ç”¨
app = Flask(__name__)
CORS(app)  # å…è¨±è·¨åŸŸè«‹æ±‚

def create_date_result_directory():
    """å‰µå»ºæŒ‰æ—¥æœŸåˆ†çµ„çš„çµæœç›®éŒ„ä¸¦è¿”å›æ™‚é–“æˆ³"""
    now = datetime.now()
    today = now.strftime("%Y%m%d")
    timestamp = now.strftime("%Y%m%d_%H%M%S")
    script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    date_result_dir = os.path.join(script_dir, "result", today)
    os.makedirs(date_result_dir, exist_ok=True)
    return date_result_dir, timestamp

def generate_reports_for_api(work_data, employee_data, result_df, senior_workloads, junior_workloads, date_result_dir, timestamp):
    """ç”Ÿæˆå®Œæ•´çš„åˆ†æå ±å‘Šï¼ˆé‡ç”¨main_manageré‚è¼¯ï¼‰"""
    try:
        # å°å…¥æ‰€éœ€æ¨¡çµ„
        import detailed_global_statistics
        import final_recommendation_report  
        import direct_calculation
        from md_report_generator import generate_md_report
        
        # åˆ‡æ›å·¥ä½œç›®éŒ„ä»¥ä½¿ç”¨ç¾æœ‰çš„å ±å‘Šç”Ÿæˆé‚è¼¯
        original_cwd = os.getcwd()
        script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        os.chdir(script_dir)
        
        try:
            # é¦–å…ˆä¿å­˜çµæœåˆ°é …ç›®æ ¹ç›®éŒ„ï¼Œè®“å…¶ä»–æ¨¡çµ„å¯ä»¥è®€å–
            temp_result_file = os.path.join(script_dir, 'result', 'result_with_assignments.csv')
            os.makedirs(os.path.dirname(temp_result_file), exist_ok=True)
            result_df.to_csv(temp_result_file, index=False, encoding='utf-8')
            
            # 1. ç”Ÿæˆè©³ç´°çµ±è¨ˆå ±å‘Šï¼ˆç›´æ¥èª¿ç”¨mainå‡½æ•¸ï¼‰
            detailed_global_statistics.main(timestamp)
            
            # ç§»å‹•ç”Ÿæˆçš„æ–‡ä»¶åˆ°æ—¥æœŸç›®éŒ„
            source_detailed_file = os.path.join(script_dir, 'result', f'detailed_statistics_report_{timestamp}.txt')
            if os.path.exists(source_detailed_file):
                dest_detailed_file = os.path.join(date_result_dir, f'detailed_statistics_report_{timestamp}.txt')
                import shutil
                shutil.move(source_detailed_file, dest_detailed_file)
                logger.info(f"è©³ç´°çµ±è¨ˆå ±å‘Šå·²ä¿å­˜åˆ°: {dest_detailed_file}")
            
            # 2. ç”ŸæˆäººåŠ›éœ€æ±‚åˆ†æï¼ˆç›´æ¥èª¿ç”¨å‡½æ•¸ä¸¦æŒ‡å®šæ™‚é–“æˆ³ï¼‰
            direct_calculation.direct_workforce_calculation(timestamp)
            
            # ç§»å‹•ç”Ÿæˆçš„æ–‡ä»¶åˆ°æ—¥æœŸç›®éŒ„
            source_workforce_file = os.path.join(script_dir, 'result', f'workforce_requirements_analysis_{timestamp}.txt')
            if os.path.exists(source_workforce_file):
                dest_workforce_file = os.path.join(date_result_dir, f'workforce_requirements_analysis_{timestamp}.txt')
                import shutil
                shutil.move(source_workforce_file, dest_workforce_file)
                logger.info(f"äººåŠ›éœ€æ±‚åˆ†æå ±å‘Šå·²ä¿å­˜åˆ°: {dest_workforce_file}")
            
            # 3. ç”ŸæˆMDæ ¼å¼å ±å‘Š
            md_file_path = generate_md_report(script_dir)
            if md_file_path:
                # ç§»å‹•MDå ±å‘Šåˆ°æ—¥æœŸç›®éŒ„ï¼Œä½¿ç”¨çµ±ä¸€æ™‚é–“æˆ³é‡å‘½å
                import shutil
                dest_md_file = os.path.join(date_result_dir, f"å·¥ä½œåˆ†é…åˆ†æå ±å‘Š_{timestamp}.md")
                shutil.move(md_file_path, dest_md_file)
                logger.info(f"MDæ ¼å¼å ±å‘Šå·²ä¿å­˜åˆ°: {dest_md_file}")
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            if os.path.exists(temp_result_file):
                os.remove(temp_result_file)
            
        finally:
            os.chdir(original_cwd)
        
        return True
        
    except Exception as e:
        logger.error(f"å ±å‘Šç”Ÿæˆå¤±æ•—: {str(e)}")
        logger.error(traceback.format_exc())
        return False

class WorkAssignmentAPI:
    """å·¥ä½œåˆ†é…APIè™•ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–APIè™•ç†å™¨"""
        self.required_work_fields = [
            'measure_record_oid', 'upload_end_time', 'promise_time', 'task_status',
            'task_status_name', 'institution_id', 'data_effective_rate', 'num_af',
            'num_pvc', 'num_sveb', 'delay_days', 'is_vip', 'is_top_job',
            'is_simple_work', 'priority', 'actual_record_days', 'source_file',
            'difficulty', 'x_value'
        ]
        
        self.required_employee_fields = ['id', 'type']
        
        self.output_fields = ['assigned_worker', 'worker_type', 'estimated_time']
    
    def validate_work_data(self, work_data):
        """é©—è­‰å·¥ä½œæ¸…å–®è³‡æ–™æ ¼å¼ï¼ˆæ”¯æŒç°¡åŒ–æ ¼å¼ï¼‰"""
        if not isinstance(work_data, list) or len(work_data) == 0:
            return False, "Work list must be a non-empty array"
        
        for i, work in enumerate(work_data):
            if not isinstance(work, dict):
                return False, f"Work item {i+1} must be an object"
            
            # æª¢æŸ¥æ ¸å¿ƒå¿…è¦æ¬„ä½ï¼ˆç°¡åŒ–é©—è­‰ï¼‰
            core_fields = ['measure_record_oid', 'priority', 'difficulty']
            missing_fields = []
            for field in core_fields:
                if field not in work:
                    missing_fields.append(field)
            
            if missing_fields:
                return False, f"Work item {i+1} is missing core fields: {', '.join(missing_fields)}"
            
            # é©—è­‰è³‡æ–™å‹åˆ¥
            try:
                # ç¢ºä¿æ•¸å€¼å‹åˆ¥æ¬„ä½æ­£ç¢º
                numeric_fields = ['priority', 'difficulty']
                for field in numeric_fields:
                    if field in work and work[field] is not None:
                        work[field] = float(work[field])
                        
            except (ValueError, TypeError) as e:
                return False, f"Work item {i+1} data type error: {str(e)}"
        
        return True, "Work list data format is correct"
    
    def validate_employee_data(self, employee_data):
        """é©—è­‰æŠ€å¸«æ¸…å–®è³‡æ–™æ ¼å¼"""
        if not isinstance(employee_data, list) or len(employee_data) == 0:
            return False, "Employee list must be a non-empty array"
        
        senior_count = 0
        junior_count = 0
        
        for i, employee in enumerate(employee_data):
            if not isinstance(employee, dict):
                return False, f"Employee item {i+1} must be an object"
            
            # æª¢æŸ¥å¿…è¦æ¬„ä½
            missing_fields = []
            for field in self.required_employee_fields:
                if field not in employee:
                    missing_fields.append(field)
            
            if missing_fields:
                return False, f"Employee item {i+1} is missing fields: {', '.join(missing_fields)}"
            
            # é©—è­‰æŠ€å¸«é¡å‹
            emp_type = employee.get('type', '').upper()
            if emp_type not in ['SENIOR', 'JUNIOR']:
                return False, f"Employee item {i+1} type must be SENIOR or JUNIOR"
            
            # çµ±è¨ˆæŠ€å¸«æ•¸é‡
            if emp_type == 'SENIOR':
                senior_count += 1
            else:
                junior_count += 1
        
        if senior_count == 0 and junior_count == 0:
            return False, "At least one employee is required"
        
        return True, f"Employee list data format is correct (Senior employees: {senior_count}, Junior employees: {junior_count})"
    
    def process_assignment(self, work_data, employee_data, external_senior_count=None, external_junior_count=None, generate_reports=False):
        """è™•ç†å·¥ä½œåˆ†é…é‚è¼¯
        
        Args:
            generate_reports: æ˜¯å¦ç”Ÿæˆå®Œæ•´å ±å‘Šï¼ˆé»˜èªFalseä»¥ä¿æŒAPIéŸ¿æ‡‰é€Ÿåº¦ï¼‰
        """
        try:
            # è½‰æ›ç‚ºDataFrame
            work_df = pd.DataFrame(work_data)
            
            logger.info(f"é–‹å§‹å·¥ä½œåˆ†é… - å·¥ä½œæ•¸é‡: {len(work_df)}")
            logger.info(f"æŠ€å¸«æ•¸é‡: {len(employee_data)}äºº")
            
            # ä½¿ç”¨é‡æ§‹å¾Œçš„åˆ†é…å‡½æ•¸
            result_df, senior_workloads, junior_workloads = assign_workers_to_tasks(
                work_data=work_df, 
                employee_data=employee_data
            )
            
            # çµ±è¨ˆçµæœ
            assigned_count = len(result_df[result_df['assigned_worker'] != 'UNASSIGNED'])
            unassigned_count = len(result_df[result_df['assigned_worker'] == 'UNASSIGNED'])
            
            logger.info(f"å·¥ä½œåˆ†é…å®Œæˆ - å·²åˆ†é…: {assigned_count}ä»¶, æœªåˆ†é…: {unassigned_count}ä»¶")
            
            # ç¢ºä¿æ‰€æœ‰æ¬„ä½éƒ½å­˜åœ¨
            if 'assigned_worker' not in result_df.columns:
                result_df['assigned_worker'] = 'UNASSIGNED'
            if 'worker_type' not in result_df.columns:
                result_df['worker_type'] = 'UNASSIGNED'
            if 'estimated_time' not in result_df.columns:
                result_df['estimated_time'] = 0
            
            # æŒ‡å®šæ¬„ä½é †åº
            desired_order = [
                "measure_record_oid", "upload_end_time", "promise_time", "task_status", "task_status_name",
                "institution_id", "data_effective_rate", "num_af", "num_pvc", "num_sveb", "delay_days",
                "is_vip", "is_top_job", "is_simple_work", "priority", "actual_record_days", "source_file",
                "difficulty", "x_value", "worker_type", "assigned_worker", "estimated_time"
            ]
            
            # åªä¿ç•™DataFrameä¸­å¯¦éš›å­˜åœ¨çš„æ¬„ä½ï¼Œä¸¦æŒ‰æŒ‡å®šé †åºæ’åˆ—
            ordered_cols = [col for col in desired_order if col in result_df.columns]
            # å¦‚æœæœ‰å…¶ä»–æ¬„ä½ä¸åœ¨desired_orderä¸­ï¼Œä¹ŸåŠ åˆ°æœ€å¾Œ
            extra_cols = [col for col in result_df.columns if col not in ordered_cols]
            final_cols = ordered_cols + extra_cols
            
            # å‰µå»ºæŒ‰æ—¥æœŸåˆ†çµ„çš„çµæœç›®éŒ„ä¸¦ç²å–æ™‚é–“æˆ³
            date_result_dir, timestamp = create_date_result_directory()
            
            # ä¿å­˜åˆ†é…çµæœCSVï¼ˆæ·»åŠ æ™‚é–“æˆ³ï¼‰
            result_file = os.path.join(date_result_dir, f'result_with_assignments_{timestamp}.csv')
            result_df[final_cols].to_csv(result_file, index=False, encoding='utf-8')
            logger.info(f"åˆ†é…çµæœå·²ä¿å­˜åˆ°: {result_file}")
            
            # ä¿å­˜çµ±è¨ˆæ‘˜è¦ï¼ˆæ·»åŠ æ™‚é–“æˆ³ï¼‰
            summary_file = os.path.join(date_result_dir, f'assignment_summary_{timestamp}.txt')
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("å·¥ä½œåˆ†é…çµ±è¨ˆæ‘˜è¦\n")
                f.write("="*50 + "\n")
                f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ™‚é–“æˆ³: {timestamp}\n\n")
                f.write(f"åŸºæœ¬çµ±è¨ˆ:\n")
                f.write(f"  ç¸½å·¥ä½œæ•¸é‡: {len(work_data)} ä»¶\n")
                f.write(f"  å·²åˆ†é…å·¥ä½œ: {assigned_count} ä»¶\n")
                f.write(f"  åˆ†é…æˆåŠŸç‡: {(assigned_count / len(work_data)) * 100:.1f}%\n\n")
                f.write(f"æŠ€å¸«å·¥ä½œè² è¼‰:\n")
                f.write(f"  è³‡æ·±æŠ€å¸«: {senior_workloads}\n")
                f.write(f"  ä¸€èˆ¬æŠ€å¸«: {junior_workloads}\n")
            logger.info(f"çµ±è¨ˆæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
            
            # å¦‚æœéœ€è¦ç”Ÿæˆå®Œæ•´å ±å‘Š
            if generate_reports:
                generate_reports_for_api(work_df, employee_data, result_df, senior_workloads, junior_workloads, date_result_dir, timestamp)
            
            # è½‰æ›å›åŸå§‹æ ¼å¼ï¼ˆæŒ‰æŒ‡å®šé †åºï¼‰
            result_data = result_df[final_cols].to_dict('records')
            
            # ç²å–ç­–ç•¥ç®¡ç†å™¨çš„çµ±è¨ˆæ‘˜è¦
            strategy_manager = get_strategy_manager(work_data=work_df, employee_data=employee_data)
            strategy_summary = strategy_manager.get_strategy_summary()
            
            return {
                'success': True,
                'data': result_data,
                'statistics': {
                    'total_tasks': len(work_data),
                    'assigned_tasks': assigned_count,
                    'unassigned_tasks': unassigned_count,
                    'assignment_rate': round((assigned_count / len(work_data)) * 100, 2),
                    'senior_workers': strategy_summary['parameters']['senior_workers'],
                    'junior_workers': strategy_summary['parameters']['junior_workers'],
                    'senior_utilization': round(strategy_summary['senior_utilization'] * 100, 2),
                    'junior_utilization': round(strategy_summary['junior_utilization'] * 100, 2),
                    'overall_utilization': round(strategy_summary['overall_utilization'] * 100, 2),
                    'leftover_senior': strategy_summary['leftover_senior'],
                    'leftover_junior': strategy_summary['leftover_junior'],
                    'meets_minimum_target': strategy_summary['meets_minimum'],
                    'minimum_target': strategy_summary['parameters']['minimum_work_target'],
                    'senior_workloads': senior_workloads,
                    'junior_workloads': junior_workloads,
                    'config_source': {
                        'senior_workers_source': 'external_api' if external_senior_count is not None else 'employee_list',
                        'junior_workers_source': 'external_api' if external_junior_count is not None else 'employee_list'
                    }
                },
                'result_directory': date_result_dir,
                'timestamp': timestamp,
                'result_files': {
                    'assignment_result': f'result_with_assignments_{timestamp}.csv',
                    'summary': f'assignment_summary_{timestamp}.txt'
                }
            }
            
        except Exception as e:
            logger.error(f"Work assignment processing failed: {str(e)}")
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'error': f"Work assignment processing failed: {str(e)}"
            }

# å»ºç«‹APIè™•ç†å™¨å¯¦ä¾‹
api_handler = WorkAssignmentAPI()

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0',
        'service': 'Assign Manager API'
    })

@app.route('/api/assign', methods=['POST'])
def assign_work():
    """
    å·¥ä½œåˆ†é…APIç«¯é»ï¼ˆæ•´åˆç‰ˆæœ¬ï¼‰
    
    æ”¯æŒå…©ç¨®åƒæ•¸æ ¼å¼ï¼š
    1. å®Œæ•´æ ¼å¼ï¼šåŒ…å«æ‰€æœ‰å¿…è¦æ¬„ä½
    2. ç°¡åŒ–æ ¼å¼ï¼šåªéœ€è¦æ ¸å¿ƒæ¬„ä½ (measure_record_oid, priority, difficulty)
    
    è«‹æ±‚æ ¼å¼:
    {
        "work_list": [
            {
                "measure_record_oid": "123",
                "priority": 1,
                "difficulty": 3,
                ... å…¶ä»–æ¬„ä½ï¼ˆå¯é¸ï¼‰
            }
        ],
        "employee_list": [
            {
                "id": "S001",
                "name": "å¼µä¸‰",
                "type": "SENIOR"
            }
        ],
        "generate_reports": true/false  // å¯é¸ï¼Œæ˜¯å¦ç”Ÿæˆå®Œæ•´å ±å‘Šï¼ˆé»˜èªfalseï¼‰
    }
    
    å›æ‡‰æ ¼å¼:
    {
        "success": true,
        "data": [ ... åŸå§‹å·¥ä½œè³‡æ–™ + assigned_worker, worker_type, estimated_time ],
        "statistics": { ... çµ±è¨ˆè³‡è¨Š },
        "result_directory": "çµæœä¿å­˜ç›®éŒ„è·¯å¾‘",
        "message": "å·¥ä½œåˆ†é…å®Œæˆ"
    }
    """
    try:
        # å–å¾—è«‹æ±‚è³‡æ–™
        request_data = request.get_json()
        
        if not request_data:
            return jsonify({
                'success': False,
                'error': 'Request data cannot be empty'
            }), 400
        
        # æå–å·¥ä½œæ¸…å–®å’ŒæŠ€å¸«æ¸…å–®ï¼ˆæ”¯æ´å¤šç¨®åƒæ•¸æ ¼å¼ï¼‰
        work_list = request_data.get('work_list', request_data.get('work_data', []))
        employee_list = request_data.get('employee_list', request_data.get('employee_data', []))
        
        # æå–å¤–éƒ¨æŠ€å¸«æ•¸é‡åƒæ•¸ (å¯é¸)
        external_senior_count = request_data.get('senior_workers_count')
        external_junior_count = request_data.get('junior_workers_count')
        
        # æ˜¯å¦ç”Ÿæˆå®Œæ•´å ±å‘Š
        generate_reports = request_data.get('generate_reports', False)
        
        # é©—è­‰å·¥ä½œæ¸…å–®ï¼ˆä½¿ç”¨ç°¡åŒ–é©—è­‰ï¼‰
        valid, message = api_handler.validate_work_data(work_list)
        if not valid:
            return jsonify({
                'success': False,
                'error': f'Work list format error: {message}'
            }), 400
        
        # é©—è­‰æŠ€å¸«æ¸…å–®
        valid, message = api_handler.validate_employee_data(employee_list)
        if not valid:
            return jsonify({
                'success': False,
                'error': f'Employee list format error: {message}'
            }), 400
        
        logger.info(f"æ”¶åˆ°å·¥ä½œåˆ†é…è«‹æ±‚ - å·¥ä½œæ•¸é‡: {len(work_list)}, æŠ€å¸«æ•¸é‡: {len(employee_list)}, ç”Ÿæˆå ±å‘Š: {generate_reports}")
        
        # åŸ·è¡Œå·¥ä½œåˆ†é…
        result = api_handler.process_assignment(
            work_list, employee_list, 
            external_senior_count, external_junior_count,
            generate_reports
        )
        
        if result['success']:
            response = {
                'success': True,
                'data': result['data'],
                'statistics': result['statistics'],
                'result_directory': result.get('result_directory', ''),
                'result_files': result.get('result_files', {}),
                'file_timestamp': result.get('timestamp', ''),
                'message': 'Work assignment completed',
                'timestamp': datetime.now().isoformat()
            }
            
            if generate_reports:
                response['message'] += ' with full reports generated'
                # å¦‚æœç”Ÿæˆäº†å®Œæ•´å ±å‘Šï¼Œæ·»åŠ å ±å‘Šæ–‡ä»¶ä¿¡æ¯
                file_timestamp = result.get('timestamp', '')
                if file_timestamp:
                    response['result_files'].update({
                        'detailed_statistics': f'detailed_statistics_report_{file_timestamp}.txt',
                        'workforce_analysis': f'workforce_requirements_analysis_{file_timestamp}.txt',
                        'md_report': f'å·¥ä½œåˆ†é…åˆ†æå ±å‘Š_{file_timestamp}.md'
                    })
            
            return jsonify(response)
        else:
            return jsonify({
                'success': False,
                'error': result['error']
            }), 500
            
    except Exception as e:
        logger.error(f"API request processing failed: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'System error: {str(e)}'
        }), 500

@app.route('/api/config', methods=['GET'])
def get_config():
    """å–å¾—ç³»çµ±é…ç½®è³‡è¨Š"""
    try:
        return jsonify({
            'success': True,
            'config': {
                'minimum_work_target': MINIMUM_WORK_TARGET,
                'work_hours_per_day': WORK_HOURS_PER_DAY,
                'senior_time': SENIOR_TIME,
                'junior_time': JUNIOR_TIME,
                'required_work_fields': api_handler.required_work_fields,
                'required_employee_fields': api_handler.required_employee_fields,
                'output_fields': api_handler.output_fields
            },
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        logger.error(f"Failed to get config: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to get config: {str(e)}'
        }), 500

@app.route('/api/test/csv', methods=['POST'])
def test_with_csv():
    """
    ä½¿ç”¨CSVæª”æ¡ˆé€²è¡Œæ¸¬è©¦çš„ç«¯é»ï¼ˆä¿ç•™åŸæœ¬åŠŸèƒ½ï¼‰
    
    è«‹æ±‚æ ¼å¼:
    {
        "work_file": "result.csv",  // å¯é¸ï¼Œé è¨­ä½¿ç”¨result.csv
        "employee_file": "employee_list.csv",  // å¯é¸ï¼Œé è¨­ä½¿ç”¨employee_list.csv
        "generate_reports": true/false  // å¯é¸ï¼Œæ˜¯å¦ç”Ÿæˆå®Œæ•´å ±å‘Šï¼ˆé»˜èªfalseï¼‰
    }
    """
    try:
        request_data = request.get_json() or {}
        
        # å–å¾—æª”æ¡ˆè·¯å¾‘
        work_file = request_data.get('work_file', 'result.csv')
        employee_file = request_data.get('employee_file', 'employee_list.csv')
        generate_reports = request_data.get('generate_reports', False)
        
        # è®€å–å·¥ä½œæ¸…å–®CSV
        work_file_path = get_data_file_path(work_file)
        if not os.path.exists(work_file_path):
            return jsonify({
                'success': False,
                'error': f'Work list file not found: {work_file_path}'
            }), 400
        
        work_df = pd.read_csv(work_file_path)
        work_list = work_df.to_dict('records')
        
        # è®€å–æŠ€å¸«æ¸…å–®CSV
        employee_manager = EmployeeManager()
        try:
            employee_manager.load_employee_list_from_csv()
            employee_dict = employee_manager.get_employee_dict()
            
            # è½‰æ›ç‚ºAPIæ ¼å¼
            employee_list = []
            for worker_id in employee_dict['senior_workers']:
                employee_list.append({
                    'id': worker_id,
                    'type': 'SENIOR'
                })
            
            for worker_id in employee_dict['junior_workers']:
                employee_list.append({
                    'id': worker_id,
                    'type': 'JUNIOR'
                })
                
        except Exception as e:
            return jsonify({
                'success': False,
                'error': f'Failed to read employee list: {str(e)}'
            }), 400
        
        logger.info(f"ä½¿ç”¨CSVæª”æ¡ˆæ¸¬è©¦ - å·¥ä½œæª”æ¡ˆ: {work_file}, æŠ€å¸«æª”æ¡ˆ: {employee_file}, ç”Ÿæˆå ±å‘Š: {generate_reports}")
        
        # åŸ·è¡Œå·¥ä½œåˆ†é…
        result = api_handler.process_assignment(work_list, employee_list, generate_reports=generate_reports)
        
        if result['success']:
            response = {
                'success': True,
                'data': result['data'],
                'statistics': result['statistics'],
                'result_directory': result.get('result_directory', ''),
                'result_files': result.get('result_files', {}),
                'file_timestamp': result.get('timestamp', ''),
                'message': 'CSV test completed successfully',
                'timestamp': datetime.now().isoformat(),
                'files_used': {
                    'work_file': work_file,
                    'employee_file': employee_file
                }
            }
            
            if generate_reports:
                response['message'] += ' with full reports generated'
                # å¦‚æœç”Ÿæˆäº†å®Œæ•´å ±å‘Šï¼Œæ·»åŠ å ±å‘Šæ–‡ä»¶ä¿¡æ¯
                file_timestamp = result.get('timestamp', '')
                if file_timestamp:
                    response['result_files'].update({
                        'detailed_statistics': f'detailed_statistics_report_{file_timestamp}.txt',
                        'workforce_analysis': f'workforce_requirements_analysis_{file_timestamp}.txt',
                        'md_report': f'å·¥ä½œåˆ†é…åˆ†æå ±å‘Š_{file_timestamp}.md'
                    })
            
            return jsonify(response)
        else:
            return jsonify({
                'success': False,
                'error': result['error']
            }), 500
            
    except Exception as e:
        logger.error(f"CSV test failed: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'CSV test failed: {str(e)}'
        }), 500

if __name__ == '__main__':
    # è¨­å®šä¼ºæœå™¨é…ç½®
    PORT = 7777
    HOST = '0.0.0.0'
    DEBUG = True
    
    print(f"ğŸš€ å·¥ä½œåˆ†é…ç®¡ç†ç³»çµ±APIä¼ºæœå™¨å•Ÿå‹•ä¸­...")
    print(f"ğŸ“¡ æœå‹™åœ°å€: http://{HOST}:{PORT}")
    print(f"ğŸ“‹ å¥åº·æª¢æŸ¥: http://{HOST}:{PORT}/health")
    print(f"ğŸ”§ APIæ–‡æª”: è«‹åƒè€ƒ API_æ–‡æª”.md")
    print(f"ğŸ› ï¸ èª¿è©¦æ¨¡å¼: {'é–‹å•Ÿ' if DEBUG else 'é—œé–‰'}")
    print("="*50)
    
    # å•Ÿå‹•Flaskæ‡‰ç”¨
    app.run(host=HOST, port=PORT, debug=DEBUG) 